#!/usr/bin/env python3
"""
Test the curriculum optimizer infrastructure with existing data
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from scripts.curriculum_optimizer import (
    load_historical_data,
    calculate_indicators,
    simulate_trades,
    suggest_params,
    LEVEL_1_CONFIG,
)

import pandas as pd
import numpy as np

def test_data_loading():
    """Test if we can load existing historical data"""
    print("\n" + "="*80)
    print("TEST 1: Data Loading")
    print("="*80)
    
    # Check what data files exist
    data_dir = Path('data/historical')
    parquet_files = list(data_dir.glob('*.parquet'))
    
    if parquet_files:
        print(f"\nâœ… Found {len(parquet_files)} existing data files:")
        for f in parquet_files[:5]:
            print(f"   - {f.name}")
        
        # Try loading one
        test_file = parquet_files[0]
        print(f"\nğŸ“‚ Loading test file: {test_file.name}")
        
        try:
            df = pd.read_parquet(test_file)
            print(f"   âœ… Loaded {len(df)} candles")
            print(f"   Columns: {list(df.columns)}")
            print(f"   Date range: {df['time'].min()} to {df['time'].max()}")
            return True
        except Exception as e:
            print(f"   âŒ Error loading: {e}")
            return False
    else:
        print("   âš ï¸  No existing data files found")
        print("   This is expected - waiting for curriculum data fetch to complete")
        return False

def test_indicator_calculation():
    """Test technical indicator calculation"""
    print("\n" + "="*80)
    print("TEST 2: Indicator Calculation")
    print("="*80)
    
    # Create synthetic data
    dates = pd.date_range('2023-01-01', periods=200, freq='5min')
    df = pd.DataFrame({
        'time': dates,
        'open': 100 + np.random.randn(200).cumsum(),
        'high': 101 + np.random.randn(200).cumsum(),
        'low': 99 + np.random.randn(200).cumsum(),
        'close': 100 + np.random.randn(200).cumsum(),
        'volume': np.random.randint(1000, 10000, 200),
    })
    
    print(f"\nğŸ“Š Created synthetic data: {len(df)} candles")
    
    try:
        df_with_indicators = calculate_indicators(df)
        print(f"   âœ… Calculated indicators")
        print(f"   New columns: {[c for c in df_with_indicators.columns if c not in df.columns]}")
        
        # Check for NaNs
        nan_count = df_with_indicators.isna().sum().sum()
        print(f"   NaN values: {nan_count} (expected for early rows)")
        
        return True
    except Exception as e:
        print(f"   âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_trade_simulation():
    """Test backtest simulation logic"""
    print("\n" + "="*80)
    print("TEST 3: Trade Simulation")
    print("="*80)
    
    # Create synthetic trending data
    np.random.seed(42)
    dates = pd.date_range('2023-01-01', periods=500, freq='5min')
    trend = np.linspace(100, 120, 500)
    noise = np.random.randn(500) * 2
    
    df = pd.DataFrame({
        'time': dates,
        'open': trend + noise,
        'high': trend + noise + abs(np.random.randn(500)),
        'low': trend + noise - abs(np.random.randn(500)),
        'close': trend + noise,
        'volume': np.random.randint(1000, 10000, 500),
    })
    
    print(f"\nğŸ“Š Created trending synthetic data: {len(df)} candles")
    
    # Test parameters
    test_params = {
        'stop_loss_pct': 1.0,
        'take_profit_pct': 2.0,
        'position_size_pct': 2.0,
    }
    
    try:
        # Test LONG
        print("\nğŸ”¹ Testing LONG strategy...")
        long_results = simulate_trades(df, test_params, direction='LONG')
        print(f"   Trades: {long_results['num_trades']}")
        print(f"   Win Rate: {long_results['win_rate']:.2%}")
        print(f"   Total Return: {long_results['total_return']:.2%}")
        print(f"   Sharpe Ratio: {long_results['sharpe_ratio']:.4f}")
        print(f"   Max Drawdown: {long_results['max_drawdown']:.2%}")
        
        # Test SHORT
        print("\nğŸ”¹ Testing SHORT strategy...")
        short_results = simulate_trades(df, test_params, direction='SHORT')
        print(f"   Trades: {short_results['num_trades']}")
        print(f"   Win Rate: {short_results['win_rate']:.2%}")
        print(f"   Total Return: {short_results['total_return']:.2%}")
        print(f"   Sharpe Ratio: {short_results['sharpe_ratio']:.4f}")
        
        if long_results['num_trades'] > 0:
            print("\n   âœ… Trade simulation working (LONG traded on uptrend)")
            return True
        else:
            print("\n   âš ï¸  No trades generated (may need parameter adjustment)")
            return False
            
    except Exception as e:
        print(f"\n   âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_parameter_suggestion():
    """Test Optuna parameter suggestion"""
    print("\n" + "="*80)
    print("TEST 4: Parameter Suggestion (Optuna Integration)")
    print("="*80)
    
    try:
        import optuna
        
        # Create a mock trial
        study = optuna.create_study(direction='maximize')
        trial = study.ask()
        
        print("\nğŸ”¹ Testing Level 1 parameter suggestion...")
        params = suggest_params(trial, level=1)
        
        print(f"   âœ… Generated parameters:")
        for key, value in params.items():
            print(f"      {key}: {value}")
        
        # Validate parameter ranges
        config = LEVEL_1_CONFIG
        all_valid = True
        
        for param_name, param_value in params.items():
            expected_range = config['param_ranges'].get(param_name)
            if expected_range:
                if isinstance(expected_range, tuple):
                    if not (expected_range[0] <= param_value <= expected_range[1]):
                        print(f"   âŒ {param_name} out of range: {param_value}")
                        all_valid = False
                elif isinstance(expected_range, list):
                    if param_value not in expected_range:
                        print(f"   âŒ {param_name} not in allowed values: {param_value}")
                        all_valid = False
        
        if all_valid:
            print(f"\n   âœ… All parameters within expected ranges")
            return True
        else:
            return False
            
    except Exception as e:
        print(f"\n   âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run all tests"""
    print("\n" + "="*80)
    print("CURRICULUM OPTIMIZER INFRASTRUCTURE TEST")
    print("="*80)
    print("\nThis test validates the optimization infrastructure is ready.")
    print("Full optimization will run when historical data fetch completes.")
    
    results = {
        'Data Loading': test_data_loading(),
        'Indicator Calculation': test_indicator_calculation(),
        'Trade Simulation': test_trade_simulation(),
        'Parameter Suggestion': test_parameter_suggestion(),
    }
    
    print("\n" + "="*80)
    print("TEST RESULTS SUMMARY")
    print("="*80)
    
    for test_name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"{status} - {test_name}")
    
    passed_count = sum(results.values())
    total_count = len(results)
    
    print(f"\nOverall: {passed_count}/{total_count} tests passed")
    
    if passed_count == total_count:
        print("\nğŸ‰ All tests passed! Infrastructure ready for optimization.")
        return 0
    elif passed_count >= total_count - 1:
        print("\nâœ… Core infrastructure working (data loading expected to fail until fetch completes)")
        return 0
    else:
        print("\nâš ï¸  Some tests failed - review errors above")
        return 1

if __name__ == '__main__':
    sys.exit(main())
