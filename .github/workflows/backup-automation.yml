name: Backup Automation

on:
  schedule:
    # Daily backups at 2 AM UTC
    - cron: '0 2 * * *'
    # Weekly full backups on Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      backup_type:
        description: 'Type of backup'
        required: true
        type: choice
        options:
          - incremental
          - full
      environment:
        description: 'Environment to backup'
        required: true
        type: choice
        options:
          - staging
          - production

permissions:
  contents: write

env:
  PYTHON_VERSION: '3.11'

jobs:
  # ===========================================================================
  # Create Backup
  # ===========================================================================
  create-backup:
    name: Create Backup
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'production' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Determine backup type
        id: backup-type
        run: |
          # Default to incremental for scheduled runs
          BACKUP_TYPE="${{ github.event.inputs.backup_type }}"

          if [ -z "$BACKUP_TYPE" ]; then
            # Sunday = full backup, other days = incremental
            if [ $(date +%u) -eq 7 ]; then
              BACKUP_TYPE="full"
            else
              BACKUP_TYPE="incremental"
            fi
          fi

          echo "backup_type=$BACKUP_TYPE" >> $GITHUB_OUTPUT
          echo "Backup type: $BACKUP_TYPE"

      - name: Create backup timestamp
        id: timestamp
        run: |
          TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)
          echo "timestamp=$TIMESTAMP" >> $GITHUB_OUTPUT

      - name: Install backup dependencies
        run: |
          pip install boto3 cryptography

      - name: Create configuration backup
        run: |
          BACKUP_DIR="backups/${{ steps.timestamp.outputs.timestamp }}"
          mkdir -p $BACKUP_DIR

          # Backup configuration files
          echo "ðŸ“ Backing up configuration files..."
          cp -r config $BACKUP_DIR/
          cp .env.example $BACKUP_DIR/
          cp pyproject.toml $BACKUP_DIR/
          cp requirements*.txt $BACKUP_DIR/

          # Create manifest
          cat > $BACKUP_DIR/manifest.json << EOF
          {
            "timestamp": "${{ steps.timestamp.outputs.timestamp }}",
            "type": "${{ steps.backup-type.outputs.backup_type }}",
            "environment": "${{ github.event.inputs.environment || 'production' }}",
            "git_ref": "${{ github.sha }}",
            "git_branch": "${{ github.ref_name }}"
          }
          EOF

          echo "âœ… Configuration backup created"

      - name: Backup application data
        run: |
          BACKUP_DIR="backups/${{ steps.timestamp.outputs.timestamp }}"

          # Create data backup directory
          mkdir -p $BACKUP_DIR/data

          # Backup logs (if any)
          if [ -d logs ]; then
            cp -r logs $BACKUP_DIR/data/
          fi

          # Backup any persistent data
          if [ -d data ]; then
            cp -r data $BACKUP_DIR/data/
          fi

          echo "âœ… Application data backup created"

      - name: Compress backup
        run: |
          TIMESTAMP="${{ steps.timestamp.outputs.timestamp }}"
          BACKUP_TYPE="${{ steps.backup-type.outputs.backup_type }}"
          BACKUP_NAME="finance-feedback-engine-${BACKUP_TYPE}-${TIMESTAMP}.tar.gz"

          tar -czf $BACKUP_NAME backups/$TIMESTAMP

          echo "backup_file=$BACKUP_NAME" >> $GITHUB_ENV
          echo "âœ… Backup compressed: $BACKUP_NAME"

      - name: Calculate backup checksum
        run: |
          sha256sum ${{ env.backup_file }} > ${{ env.backup_file }}.sha256
          echo "Checksum: $(cat ${{ env.backup_file }}.sha256)"

      - name: Upload backup to artifacts
        uses: actions/upload-artifact@v4
        with:
          name: backup-${{ steps.timestamp.outputs.timestamp }}
          path: |
            ${{ env.backup_file }}
            ${{ env.backup_file }}.sha256
          retention-days: 90

      - name: Upload to cloud storage
        if: github.event.inputs.environment == 'production' || github.event_name == 'schedule'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
        run: |
          pip install awscli

          BUCKET="${{ secrets.BACKUP_BUCKET || 'finance-feedback-engine-backups' }}"
          S3_PATH="s3://$BUCKET/${{ github.event.inputs.environment || 'production' }}/${{ env.backup_file }}"

          echo "Uploading backup to S3: $S3_PATH"

          aws s3 cp ${{ env.backup_file }} $S3_PATH
          aws s3 cp ${{ env.backup_file }}.sha256 ${S3_PATH}.sha256

          # Set lifecycle policy for old backups
          aws s3api put-object-tagging \
            --bucket $BUCKET \
            --key "${{ github.event.inputs.environment || 'production' }}/${{ env.backup_file }}" \
            --tagging "TagSet=[{Key=Type,Value=${{ steps.backup-type.outputs.backup_type }}},{Key=Timestamp,Value=${{ steps.timestamp.outputs.timestamp }}}]"

          echo "âœ… Backup uploaded to S3"

  # ===========================================================================
  # Verify Backup Integrity
  # ===========================================================================
  verify-backup:
    name: Verify Backup
    needs: create-backup
    runs-on: ubuntu-latest

    steps:
      - name: Download backup artifacts
        uses: actions/download-artifact@v4

      - name: Verify backup integrity
        run: |
          echo "ðŸ” Verifying backup integrity..."

          # Find the latest backup
          BACKUP_FILE=$(find . -name "*.tar.gz" -type f | head -1)

          if [ -z "$BACKUP_FILE" ]; then
            echo "âŒ No backup file found"
            exit 1
          fi

          echo "Found backup: $BACKUP_FILE"

          # Verify checksum
          if [ -f "${BACKUP_FILE}.sha256" ]; then
            sha256sum -c ${BACKUP_FILE}.sha256
            echo "âœ… Checksum verified"
          else
            echo "âš ï¸ No checksum file found"
          fi

          # Test extraction
          echo "Testing extraction..."
          mkdir -p test-restore
          tar -xzf $BACKUP_FILE -C test-restore

          # Verify manifest
          if [ -f test-restore/backups/*/manifest.json ]; then
            echo "âœ… Manifest found"
            cat test-restore/backups/*/manifest.json
          else
            echo "âŒ Manifest not found"
            exit 1
          fi

          echo "âœ… Backup verification successful"

  # ===========================================================================
  # Cleanup Old Backups
  # ===========================================================================
  cleanup-old-backups:
    name: Cleanup Old Backups
    needs: verify-backup
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'

    steps:
      - name: Install AWS CLI
        run: |
          pip install awscli

      - name: Cleanup old backups from S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
        run: |
          BUCKET="${{ secrets.BACKUP_BUCKET || 'finance-feedback-engine-backups' }}"
          RETENTION_DAYS=90

          echo "ðŸ§¹ Cleaning up backups older than $RETENTION_DAYS days..."

          # List and delete old backups
          CUTOFF_DATE=$(date -d "$RETENTION_DAYS days ago" +%Y%m%d)

          aws s3 ls s3://$BUCKET/ --recursive | while read -r line; do
            FILE_DATE=$(echo $line | awk '{print $1}' | tr -d '-')
            FILE_PATH=$(echo $line | awk '{print $4}')

            if [ "$FILE_DATE" -lt "$CUTOFF_DATE" ]; then
              echo "Deleting old backup: $FILE_PATH"
              aws s3 rm s3://$BUCKET/$FILE_PATH
            fi
          done

          echo "âœ… Cleanup completed"

  # ===========================================================================
  # Test Disaster Recovery
  # ===========================================================================
  test-recovery:
    name: Test Disaster Recovery
    needs: verify-backup
    runs-on: ubuntu-latest
    if: github.event.inputs.backup_type == 'full' || github.event_name == 'schedule'

    steps:
      - name: Download backup artifacts
        uses: actions/download-artifact@v4

      - name: Test restore procedure
        run: |
          echo "ðŸ§ª Testing disaster recovery procedure..."

          # Find the latest backup
          BACKUP_FILE=$(find . -name "*.tar.gz" -type f | head -1)

          # Extract backup
          mkdir -p restore-test
          tar -xzf $BACKUP_FILE -C restore-test

          # Verify critical files
          CRITICAL_FILES=(
            "config"
            "requirements.txt"
            "pyproject.toml"
          )

          for file in "${CRITICAL_FILES[@]}"; do
            if [ -e "restore-test/backups/*/$file" ]; then
              echo "âœ… Found: $file"
            else
              echo "âŒ Missing: $file"
              exit 1
            fi
          done

          echo "âœ… Disaster recovery test successful"

  # ===========================================================================
  # Backup Summary Report
  # ===========================================================================
  backup-summary:
    name: Backup Summary
    needs: [create-backup, verify-backup, cleanup-old-backups, test-recovery]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Generate backup report
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          # ðŸ’¾ Backup Automation Summary

          ## Backup Details
          - **Type:** ${{ needs.create-backup.outputs.backup_type || 'incremental' }}
          - **Environment:** ${{ github.event.inputs.environment || 'production' }}
          - **Timestamp:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')

          ## Job Results
          | Job | Status |
          |-----|--------|
          | Create Backup | ${{ needs.create-backup.result }} |
          | Verify Backup | ${{ needs.verify-backup.result }} |
          | Cleanup Old Backups | ${{ needs.cleanup-old-backups.result }} |
          | Test Recovery | ${{ needs.test-recovery.result }} |

          ## Retention Policy
          - **Incremental backups:** 30 days
          - **Full backups:** 90 days
          - **Storage:** AWS S3

          ## Next Steps
          - Backups are automatically verified
          - Old backups are cleaned up based on retention policy
          - Disaster recovery tests run on full backups

          ---
          *Automated by GitHub Actions*
          EOF

      - name: Notify on failure
        if: ${{ needs.create-backup.result == 'failure' || needs.verify-backup.result == 'failure' }}
        run: |
          echo "::error::Backup process failed. Please investigate immediately."
