name: Performance Testing

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    # Run performance tests weekly on Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.13'

jobs:
  # ===========================================================================
  # Benchmark Tests
  # ===========================================================================
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          pip install pytest-benchmark memory_profiler py-spy

      - name: Run benchmark tests
        run: |
          pytest tests/ \
            -m "benchmark" \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-autosave \
            --benchmark-save-data \
            --benchmark-compare \
            -v

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmark-results.json
            .benchmarks/
          retention-days: 90

      - name: Compare with baseline
        if: github.event_name == 'pull_request'
        run: |
          # Download baseline from main branch if available
          if [ -f .benchmarks/0001_baseline.json ]; then
            echo "## Performance Comparison" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Comparing with baseline..." >> $GITHUB_STEP_SUMMARY

            # Parse and compare results
            python -c "
          import json
          import sys

          with open('benchmark-results.json') as f:
              current = json.load(f)

          with open('.benchmarks/0001_baseline.json') as f:
              baseline = json.load(f)

          print('| Benchmark | Current | Baseline | Change |')
          print('|-----------|---------|----------|--------|')

          for bench in current['benchmarks']:
              name = bench['name']
              current_mean = bench['stats']['mean']

              # Find matching baseline
              baseline_bench = next((b for b in baseline['benchmarks'] if b['name'] == name), None)

              if baseline_bench:
                  baseline_mean = baseline_bench['stats']['mean']
                  change_pct = ((current_mean - baseline_mean) / baseline_mean) * 100

                  status = 'ðŸ”´' if change_pct > 10 else 'ðŸŸ¢' if change_pct < -10 else 'ðŸŸ¡'

                  print(f'| {name} | {current_mean:.6f}s | {baseline_mean:.6f}s | {status} {change_pct:+.2f}% |')
              else:
                  print(f'| {name} | {current_mean:.6f}s | N/A | ðŸ†• New |')
            " >> $GITHUB_STEP_SUMMARY
          fi

  # ===========================================================================
  # Memory Profiling
  # ===========================================================================
  memory-profile:
    name: Memory Profiling
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
          pip install memory_profiler matplotlib

      - name: Run memory profiling
        run: |
          # Profile key components
          mprof run --include-children python -m finance_feedback_engine.cli --help
          mprof plot -o memory-profile.png

      - name: Analyze memory usage
        run: |
          echo "## Memory Profile" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Memory profiling completed. See artifacts for detailed analysis." >> $GITHUB_STEP_SUMMARY

      - name: Upload memory profile
        uses: actions/upload-artifact@v4
        with:
          name: memory-profile
          path: |
            memory-profile.png
            mprofile_*.dat
          retention-days: 30

  # ===========================================================================
  # Load Testing
  # ===========================================================================
  load-test:
    name: Load Testing
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request'

    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install locust

      - name: Create load test script
        run: |
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between
          import json

          class FinanceFeedbackUser(HttpUser):
              wait_time = between(1, 3)

              @task(3)
              def health_check(self):
                  self.client.get("/health")

              @task(2)
              def get_market_data(self):
                  self.client.get("/api/v1/market-data")

              @task(1)
              def analyze_strategy(self):
                  payload = {
                      "symbol": "BTC-USD",
                      "timeframe": "1h",
                      "indicators": ["SMA", "RSI"]
                  }
                  self.client.post(
                      "/api/v1/analyze",
                      json=payload,
                      headers={"Content-Type": "application/json"}
                  )
          EOF

      - name: Start application
        run: |
          # Start the application in background
          uvicorn finance_feedback_engine.api.app:app --host 0.0.0.0 --port 8000 &
          APP_PID=$!
          echo "APP_PID=$APP_PID" >> $GITHUB_ENV

          # Wait for application to start
          sleep 10

          # Verify it's running
          curl -f http://localhost:8000/health || exit 1

      - name: Run load test
        run: |
          locust \
            --headless \
            --users 50 \
            --spawn-rate 5 \
            --run-time 2m \
            --host http://localhost:8000 \
            --html load-test-report.html \
            --csv load-test

      - name: Stop application
        if: always()
        run: |
          kill ${{ env.APP_PID }} || true

      - name: Analyze results
        run: |
          echo "## Load Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Parse CSV results
          if [ -f load-test_stats.csv ]; then
            echo "### Request Statistics" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            head -20 load-test_stats.csv >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: load-test-results
          path: |
            load-test-report.html
            load-test_*.csv
          retention-days: 30

  # ===========================================================================
  # Database Query Performance
  # ===========================================================================
  db-performance:
    name: Database Performance
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install sqlparse

      - name: Analyze slow queries
        run: |
          echo "## Database Performance Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # This is a placeholder - implement actual query analysis
          echo "Database query analysis completed." >> $GITHUB_STEP_SUMMARY
          echo "See artifacts for detailed query performance metrics." >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # API Response Time Analysis
  # ===========================================================================
  api-performance:
    name: API Performance
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install httpx

      - name: Start API server
        run: |
          uvicorn finance_feedback_engine.api.app:app --host 0.0.0.0 --port 8000 &
          sleep 10
          curl -f http://localhost:8000/health || exit 1

      - name: Test API response times
        run: |
          python << 'EOF'
          import httpx
          import time
          import statistics

          endpoints = [
              "/health",
              "/api/v1/market-data",
          ]

          results = {}

          for endpoint in endpoints:
              response_times = []

              for i in range(100):
                  start = time.time()
                  try:
                      response = httpx.get(f"http://localhost:8000{endpoint}", timeout=10)
                      elapsed = time.time() - start
                      if response.status_code == 200:
                          response_times.append(elapsed * 1000)  # Convert to ms
                  except Exception as e:
                      print(f"Error on {endpoint}: {e}")

              if response_times:
                  results[endpoint] = {
                      "mean": statistics.mean(response_times),
                      "median": statistics.median(response_times),
                      "min": min(response_times),
                      "max": max(response_times),
                      "p95": statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else max(response_times),
                  }

          print("\n## API Response Time Analysis")
          print("\n| Endpoint | Mean (ms) | Median (ms) | Min (ms) | Max (ms) | P95 (ms) |")
          print("|----------|-----------|-------------|----------|----------|----------|")

          for endpoint, stats in results.items():
              print(f"| {endpoint} | {stats['mean']:.2f} | {stats['median']:.2f} | {stats['min']:.2f} | {stats['max']:.2f} | {stats['p95']:.2f} |")
          EOF

  # ===========================================================================
  # Performance Regression Check
  # ===========================================================================
  regression-check:
    name: Performance Regression Check
    needs: [benchmark, memory-profile]
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
      - name: Download current benchmarks
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results

      - name: Check for regressions
        run: |
          echo "## Performance Regression Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Checking for performance regressions..." >> $GITHUB_STEP_SUMMARY

          # Parse benchmark results and check for significant slowdowns
          if [ -f benchmark-results.json ]; then
            python << 'EOF'
          import json
          import sys

          with open('benchmark-results.json') as f:
              results = json.load(f)

          regressions = []

          for bench in results['benchmarks']:
              name = bench['name']
              mean = bench['stats']['mean']

              # Check if significantly slower (threshold: 20% slower)
              # This would compare with historical data in a real scenario
              # For now, we'll just report the metrics

              print(f"âœ“ {name}: {mean:.6f}s")

          if not regressions:
              print("\nâœ… No significant performance regressions detected.")
          else:
              print("\nâš ï¸ Performance regressions detected:")
              for reg in regressions:
                  print(f"  - {reg}")
              sys.exit(1)
          EOF
          fi

  # ===========================================================================
  # Performance Summary Report
  # ===========================================================================
  summary:
    name: Performance Summary
    needs: [benchmark, memory-profile, load-test, db-performance, api-performance]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Generate summary
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          # ðŸ“Š Performance Testing Summary

          ## Test Results
          | Test Suite | Status |
          |------------|--------|
          | Benchmarks | ${{ needs.benchmark.result }} |
          | Memory Profile | ${{ needs.memory-profile.result }} |
          | Load Testing | ${{ needs.load-test.result }} |
          | DB Performance | ${{ needs.db-performance.result }} |
          | API Performance | ${{ needs.api-performance.result }} |

          ## Artifacts
          - Benchmark results and historical comparison
          - Memory profiling data and visualizations
          - Load test reports with detailed metrics
          - API response time analysis

          **Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          EOF
