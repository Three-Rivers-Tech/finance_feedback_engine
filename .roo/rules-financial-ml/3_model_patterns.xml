<ml_model_patterns>
  <overview>
    Common model architectures and implementation patterns for financial ML tasks.
    These patterns are proven in production and follow best practices for the
    Finance Feedback Engine project.
  </overview>

  <pattern name="time_series_lstm">
    <use_case>Sequential price prediction with long-term dependencies</use_case>
    <description>
      LSTM networks excel at learning temporal patterns in financial data,
      capturing both short-term fluctuations and longer-term trends.
    </description>
    <architecture>
      <layer>Input: sequence of features (OHLCV, indicators) over lookback window</layer>
      <layer>LSTM layers: 2-3 stacked layers with 64-256 units</layer>
      <layer>Dropout: 0.2-0.3 between layers for regularization</layer>
      <layer>Dense layers: 1-2 fully connected layers</layer>
      <layer>Output: price prediction or classification (buy/sell/hold)</layer>
    </architecture>
    <implementation><![CDATA[
import torch
import torch.nn as nn

class FinancialLSTM(nn.Module):
    """LSTM model for financial time series prediction."""

    def __init__(self, input_size, hidden_size=128, num_layers=2,
                 output_size=1, dropout=0.2):
        super(FinancialLSTM, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM layers
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )

        # Dropout for regularization
        self.dropout = nn.Dropout(dropout)

        # Fully connected layers
        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size // 2, output_size)

    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)

        # LSTM forward pass
        lstm_out, (hidden, cell) = self.lstm(x)

        # Take output from last time step
        last_output = lstm_out[:, -1, :]

        # Fully connected layers
        out = self.dropout(last_output)
        out = self.fc1(out)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)

        return out

# Training loop
def train_lstm_model(model, train_loader, val_loader, num_epochs=100):
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )

    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0

    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()

        train_loss /= len(train_loader)
        val_loss /= len(val_loader)

        # Learning rate scheduling
        scheduler.step(val_loss)

        # Early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), 'best_lstm_model.pt')
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch}")
                break

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Train Loss = {train_loss:.6f}, "
                  f"Val Loss = {val_loss:.6f}")

    return model
    ]]></implementation>
    <hyperparameters>
      <param name="sequence_length">20-60 time steps (lookback window)</param>
      <param name="hidden_size">64-256 units</param>
      <param name="num_layers">2-3 LSTM layers</param>
      <param name="dropout">0.2-0.3</param>
      <param name="learning_rate">0.001 with ReduceLROnPlateau</param>
      <param name="batch_size">32-128</param>
    </hyperparameters>
  </pattern>

  <pattern name="gradient_boosting_classification">
    <use_case>Feature-based buy/sell/hold classification</use_case>
    <description>
      Gradient boosting machines (XGBoost, LightGBM, CatBoost) are highly effective
      for tabular financial data with engineered features.
    </description>
    <implementation><![CDATA[
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np

class GradientBoostingTrader:
    """Gradient boosting model for trading signals."""

    def __init__(self, model_type='xgboost'):
        self.model_type = model_type
        self.model = None
        self.feature_importance = None

    def create_features(self, df):
        """Create technical indicator features."""
        features = pd.DataFrame(index=df.index)

        # Price-based features
        features['returns_1d'] = df['close'].pct_change()
        features['returns_5d'] = df['close'].pct_change(5)
        features['returns_20d'] = df['close'].pct_change(20)

        # Moving averages
        features['sma_20'] = df['close'].rolling(20).mean()
        features['sma_50'] = df['close'].rolling(50).mean()
        features['price_to_sma20'] = df['close'] / features['sma_20']
        features['price_to_sma50'] = df['close'] / features['sma_50']

        # Volatility
        features['volatility_20'] = df['close'].pct_change().rolling(20).std()
        features['volatility_50'] = df['close'].pct_change().rolling(50).std()

        # Volume features
        features['volume_sma_20'] = df['volume'].rolling(20).mean()
        features['volume_ratio'] = df['volume'] / features['volume_sma_20']

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        features['rsi_14'] = 100 - (100 / (1 + rs))

        # MACD
        ema_12 = df['close'].ewm(span=12).mean()
        ema_26 = df['close'].ewm(span=26).mean()
        features['macd'] = ema_12 - ema_26
        features['macd_signal'] = features['macd'].ewm(span=9).mean()

        # Bollinger Bands
        sma = df['close'].rolling(20).mean()
        std = df['close'].rolling(20).std()
        features['bb_upper'] = sma + (2 * std)
        features['bb_lower'] = sma - (2 * std)
        features['bb_position'] = (df['close'] - features['bb_lower']) / \
                                   (features['bb_upper'] - features['bb_lower'])

        return features.dropna()

    def create_labels(self, df, threshold=0.02):
        """Create buy/sell/hold labels based on future returns."""
        # Calculate future return (next day)
        future_return = df['close'].shift(-1) / df['close'] - 1

        # Create labels: 0=SELL, 1=HOLD, 2=BUY
        labels = np.where(future_return > threshold, 2,
                         np.where(future_return < -threshold, 0, 1))

        return labels[:-1]  # Remove last element (no future data)

    def train(self, df, test_size=0.2):
        """Train gradient boosting model."""
        # Create features and labels
        features = self.create_features(df)
        labels = self.create_labels(df)

        # Align features and labels
        min_len = min(len(features), len(labels))
        X = features.iloc[:min_len]
        y = labels[:min_len]

        # Time-based split
        split_idx = int(len(X) * (1 - test_size))
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]

        # Train model based on type
        if self.model_type == 'xgboost':
            self.model = xgb.XGBClassifier(
                n_estimators=200,
                max_depth=5,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                objective='multi:softmax',
                num_class=3,
                random_state=42
            )
        elif self.model_type == 'lightgbm':
            self.model = lgb.LGBMClassifier(
                n_estimators=200,
                max_depth=5,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                objective='multiclass',
                num_class=3,
                random_state=42
            )

        # Fit model
        self.model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            early_stopping_rounds=20,
            verbose=False
        )

        # Evaluate
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')

        print(f"Test Accuracy: {accuracy:.4f}")
        print(f"Test F1 Score: {f1:.4f}")
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred,
                                    target_names=['SELL', 'HOLD', 'BUY']))

        # Feature importance
        self.feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)

        return self.model

    def predict(self, features):
        """Make prediction for new data."""
        if self.model is None:
            raise ValueError("Model not trained yet")

        prediction = self.model.predict(features)
        probabilities = self.model.predict_proba(features)

        return {
            'action': ['SELL', 'HOLD', 'BUY'][prediction[0]],
            'confidence': float(probabilities[0].max()),
            'probabilities': {
                'SELL': float(probabilities[0][0]),
                'HOLD': float(probabilities[0][1]),
                'BUY': float(probabilities[0][2])
            }
        }
    ]]></implementation>
    <hyperparameters>
      <param name="n_estimators">100-500 trees</param>
      <param name="max_depth">3-7</param>
      <param name="learning_rate">0.01-0.1</param>
      <param name="subsample">0.7-0.9</param>
      <param name="colsample_bytree">0.7-0.9</param>
    </hyperparameters>
    <advantages>
      <advantage>Fast training and inference</advantage>
      <advantage>Handles missing values automatically</advantage>
      <advantage>Built-in feature importance</advantage>
      <advantage>Resistant to overfitting with proper tuning</advantage>
    </advantages>
  </pattern>

  <pattern name="ensemble_stacking">
    <use_case>Combining multiple models for robust predictions</use_case>
    <description>
      Stack multiple models to leverage their complementary strengths.
      Integrates with existing ensemble manager in decision engine.
    </description>
    <implementation><![CDATA[
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier

class StackedEnsemble:
    """Stacked ensemble for trading decisions."""

    def __init__(self):
        # Base models
        self.base_models = [
            ('xgb', xgb.XGBClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            )),
            ('lgb', lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=5,
                learning_rate=0.1,
                random_state=42
            )),
            ('rf', RandomForestClassifier(
                n_estimators=100,
                max_depth=7,
                random_state=42
            ))
        ]

        # Meta-learner (final layer)
        self.meta_learner = LogisticRegression(
            max_iter=1000,
            random_state=42
        )

        # Stacking classifier
        self.model = StackingClassifier(
            estimators=self.base_models,
            final_estimator=self.meta_learner,
            cv=5,  # Cross-validation folds
            n_jobs=-1
        )

    def train(self, X_train, y_train, X_val, y_val):
        """Train stacked ensemble."""
        print("Training stacked ensemble...")

        # Fit model
        self.model.fit(X_train, y_train)

        # Evaluate on validation set
        val_score = self.model.score(X_val, y_val)
        print(f"Validation Accuracy: {val_score:.4f}")

        # Individual model scores
        for name, estimator in self.base_models:
            estimator.fit(X_train, y_train)
            score = estimator.score(X_val, y_val)
            print(f"{name} Accuracy: {score:.4f}")

        return self.model

    def predict_with_confidence(self, X):
        """Predict with ensemble confidence."""
        # Get predictions from all base models
        base_predictions = []
        for name, estimator in self.base_models:
            pred = estimator.predict(X)
            base_predictions.append(pred)

        # Final prediction
        final_pred = self.model.predict(X)
        final_proba = self.model.predict_proba(X)

        # Calculate agreement score
        agreement = sum(pred == final_pred for pred in base_predictions) / len(base_predictions)

        return {
            'action': ['SELL', 'HOLD', 'BUY'][final_pred[0]],
            'confidence': float(final_proba[0].max()) * agreement[0],
            'base_agreement': float(agreement[0]),
            'probabilities': {
                'SELL': float(final_proba[0][0]),
                'HOLD': float(final_proba[0][1]),
                'BUY': float(final_proba[0][2])
            }
        }
    ]]></implementation>
    <integration_with_ensemble_manager>
      <note>
        This stacking pattern complements the existing EnsembleManager
        in decision_engine/ensemble_manager.py which handles AI provider
        aggregation. Consider creating a new MLEnsembleProvider that
        uses this stacking approach internally.
      </note>
    </integration_with_ensemble_manager>
  </pattern>

  <pattern name="reinforcement_learning_agent">
    <use_case>Learning optimal trading policies through environment interaction</use_case>
    <description>
      Reinforcement learning agent that learns to maximize portfolio value
      through trial and error in a simulated trading environment.
    </description>
    <implementation><![CDATA[
import gym
from gym import spaces
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

class TradingEnvironment(gym.Env):
    """Custom trading environment for RL agent."""

    def __init__(self, df, initial_balance=10000, transaction_cost=0.001):
        super(TradingEnvironment, self).__init__()

        self.df = df
        self.initial_balance = initial_balance
        self.transaction_cost = transaction_cost

        # State space: [balance, position, price, features...]
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(len(df.columns) + 3,),  # +3 for balance, position, price
            dtype=np.float32
        )

        # Action space: [SELL, HOLD, BUY]
        self.action_space = spaces.Discrete(3)

        self.reset()

    def reset(self):
        """Reset environment to initial state."""
        self.current_step = 0
        self.balance = self.initial_balance
        self.position = 0  # Number of shares held
        self.entry_price = 0

        return self._get_observation()

    def _get_observation(self):
        """Get current state observation."""
        row = self.df.iloc[self.current_step]
        price = row['close']

        # Construct state vector
        state = np.array([
            self.balance / self.initial_balance,  # Normalized balance
            self.position,
            price,
            *row.values  # All features
        ], dtype=np.float32)

        return state

    def step(self, action):
        """Execute action and return new state, reward, done."""
        row = self.df.iloc[self.current_step]
        price = row['close']

        # Execute action
        if action == 0:  # SELL
            if self.position > 0:
                # Sell all shares
                sale_value = self.position * price * (1 - self.transaction_cost)
                self.balance += sale_value
                reward = sale_value - (self.position * self.entry_price)
                self.position = 0
                self.entry_price = 0
            else:
                reward = 0  # No action taken

        elif action == 2:  # BUY
            if self.position == 0 and self.balance > 0:
                # Buy shares with 90% of balance
                buy_amount = self.balance * 0.9
                cost = buy_amount * (1 + self.transaction_cost)
                self.position = buy_amount / price
                self.balance -= cost
                self.entry_price = price
                reward = 0  # No immediate reward
            else:
                reward = 0  # Already have position or no balance

        else:  # HOLD
            # Reward based on unrealized P&L
            if self.position > 0:
                unrealized_pnl = self.position * (price - self.entry_price)
                reward = unrealized_pnl / self.initial_balance
            else:
                reward = 0

        # Move to next step
        self.current_step += 1
        done = self.current_step >= len(self.df) - 1

        # Get new observation
        obs = self._get_observation()

        # Calculate portfolio value for info
        portfolio_value = self.balance + (self.position * price)
        info = {'portfolio_value': portfolio_value}

        return obs, reward, done, info

    def render(self, mode='human'):
        """Render current state."""
        row = self.df.iloc[self.current_step]
        price = row['close']
        portfolio_value = self.balance + (self.position * price)

        print(f"Step: {self.current_step}")
        print(f"Price: ${price:.2f}")
        print(f"Balance: ${self.balance:.2f}")
        print(f"Position: {self.position:.4f} shares")
        print(f"Portfolio Value: ${portfolio_value:.2f}")
        print(f"Return: {(portfolio_value / self.initial_balance - 1) * 100:.2f}%")

def train_rl_agent(df, total_timesteps=100000):
    """Train RL agent using PPO algorithm."""
    # Create environment
    env = DummyVecEnv([lambda: TradingEnvironment(df)])

    # Create PPO model
    model = PPO(
        'MlpPolicy',
        env,
        learning_rate=0.0003,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        verbose=1,
        tensorboard_log="./ppo_trading_tensorboard/"
    )

    # Train model
    model.learn(total_timesteps=total_timesteps)

    # Save model
    model.save("ppo_trading_agent")

    return model

def evaluate_rl_agent(model, df):
    """Evaluate trained RL agent."""
    env = TradingEnvironment(df)
    obs = env.reset()

    done = False
    total_reward = 0

    while not done:
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, done, info = env.step(action)
        total_reward += reward

    final_value = info['portfolio_value']
    return_pct = (final_value / env.initial_balance - 1) * 100

    print(f"Final Portfolio Value: ${final_value:.2f}")
    print(f"Total Return: {return_pct:.2f}%")

    return final_value, return_pct
    ]]></implementation>
    <considerations>
      <consideration>Define reward function carefully to encourage desired behavior</consideration>
      <consideration>Use realistic transaction costs and slippage</consideration>
      <consideration>Train on diverse market conditions</consideration>
      <consideration>Implement proper risk management constraints</consideration>
      <consideration>Monitor for overtrading (excessive actions)</consideration>
    </considerations>
  </pattern>

  <integration_guidelines>
    <guideline name="decision_engine_integration">
      <description>Integrate ML models with existing decision engine</description>
      <steps>
        <step>Create new AI provider class inheriting from base provider</step>
        <step>Implement query() method that returns structured decision</step>
        <step>Add provider to ensemble manager configuration</step>
        <step>Set appropriate weights for ensemble voting</step>
        <step>Test with mock data before production deployment</step>
      </steps>
      <reference>
        See finance_feedback_engine/decision_engine/ensemble_manager.py
        for existing provider patterns
      </reference>
    </guideline>

    <guideline name="backtesting_integration">
      <description>Test ML models using project's backtesting framework</description>
      <commands>
        <command>python main.py backtest ASSET --provider ml_model</command>
        <command>python main.py walk-forward ASSET --train-ratio 0.7</command>
        <command>python main.py monte-carlo ASSET --simulations 500</command>
      </commands>
    </guideline>

    <guideline name="monitoring_integration">
      <description>Integrate with live trade monitoring</description>
      <steps>
        <step>Log all predictions with timestamps</step>
        <step>Track prediction accuracy vs actual outcomes</step>
        <step>Monitor confidence scores distribution</step>
        <step>Detect model drift using statistical tests</step>
        <step>Alert on anomalous predictions</step>
      </steps>
      <reference>
        See finance_feedback_engine/monitoring/trade_monitor.py and
        finance_feedback_engine/memory/portfolio_memory.py
      </reference>
    </guideline>
  </integration_guidelines>
</ml_model_patterns>
