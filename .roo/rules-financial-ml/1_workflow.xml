<ml_workflow_instructions>
  <mode_overview>
    The Financial ML Specialist mode guides you through the complete machine learning workflow
    for financial trading systems, from data preparation through model deployment and monitoring.
    This mode emphasizes rigorous validation, prevention of overfitting, and production-ready implementations.
  </mode_overview>

  <initialization_steps>
    <step number="1">
      <action>Understand the ML objective</action>
      <details>
        Clarify the specific ML task:
        - Price prediction (regression)
        - Direction classification (buy/sell/hold)
        - Regime detection (trending/ranging/volatile)
        - Risk estimation (volatility forecasting)
        - Portfolio optimization (allocation weights)
        - Reinforcement learning (trading agent)
      </details>
      <validation>
        Ensure the problem is well-defined with clear success metrics
      </validation>
    </step>

    <step number="2">
      <action>Gather context about existing implementations</action>
      <tools>
        <tool>codebase_search - Find related ML models and decision engines</tool>
        <tool>read_file - Examine existing ensemble manager and agent implementations</tool>
        <tool>list_files - Understand data provider and feature engineering structure</tool>
      </tools>
      <focus_areas>
        <area>finance_feedback_engine/decision_engine/ - AI provider integration</area>
        <area>finance_feedback_engine/agent/ - Autonomous trading agent</area>
        <area>finance_feedback_engine/memory/ - Portfolio memory and learning</area>
        <area>finance_feedback_engine/data_providers/ - Market data and features</area>
        <area>tests/test_ensemble*.py - Ensemble testing patterns</area>
      </focus_areas>
    </step>

    <step number="3">
      <action>Assess data availability and quality</action>
      <checks>
        <check>Verify data sources (Alpha Vantage, platform APIs)</check>
        <check>Check data freshness and update frequency</check>
        <check>Identify missing data and gaps</check>
        <check>Evaluate feature availability (OHLCV, sentiment, technical indicators)</check>
        <check>Review existing preprocessing pipelines</check>
      </checks>
    </step>
  </initialization_steps>

  <main_workflow>
    <phase name="data_preparation">
      <description>Prepare clean, validated data for model training</description>
      <steps>
        <step priority="critical">
          <name>Data quality validation</name>
          <actions>
            <action>Check for missing values and outliers</action>
            <action>Validate timestamp consistency</action>
            <action>Ensure no future data leakage</action>
            <action>Verify data alignment across timeframes</action>
          </actions>
          <antipatterns>
            <antipattern>Using forward-filled data without flagging</antipattern>
            <antipattern>Ignoring gaps in market hours</antipattern>
            <antipattern>Mixing timezones</antipattern>
          </antipatterns>
        </step>

        <step priority="high">
          <name>Feature engineering</name>
          <actions>
            <action>Generate technical indicators (RSI, MACD, Bollinger Bands, ADX, ATR)</action>
            <action>Create cross-timeframe features (1m, 5m, 15m, 1h, 4h, daily)</action>
            <action>Engineer sentiment features from news data</action>
            <action>Calculate volume-based features</action>
            <action>Create lagged features for time series</action>
            <action>Add market regime indicators</action>
          </actions>
          <considerations>
            <consideration>Avoid look-ahead bias - only use past data</consideration>
            <consideration>Handle different market regimes (trending vs ranging)</consideration>
            <consideration>Account for market microstructure (spreads, slippage)</consideration>
          </considerations>
        </step>

        <step priority="high">
          <name>Train/test/validation split</name>
          <strategy>
            Use walk-forward analysis or time-based splits:
            - Training: Historical data (e.g., 70% of time range)
            - Validation: Out-of-sample period for hyperparameter tuning
            - Test: Final holdout for performance evaluation

            Never shuffle time series data - maintain temporal order
          </strategy>
          <best_practice>
            Implement expanding window or rolling window cross-validation
            for realistic performance estimation
          </best_practice>
        </step>
      </steps>
    </phase>

    <phase name="model_development">
      <description>Design, train, and optimize ML models</description>
      <steps>
        <step priority="critical">
          <name>Model architecture selection</name>
          <options>
            <option type="time_series">
              <models>LSTM, GRU, Transformers, Temporal Convolutional Networks</models>
              <use_case>Sequential price prediction with long-term dependencies</use_case>
            </option>
            <option type="tabular">
              <models>XGBoost, LightGBM, CatBoost, Random Forest, Gradient Boosting</models>
              <use_case>Feature-based classification or regression</use_case>
            </option>
            <option type="ensemble">
              <models>Stacking, blending, weighted averaging</models>
              <use_case>Combining multiple models for robustness</use_case>
            </option>
            <option type="reinforcement">
              <models>PPO, A2C, DDPG, SAC</models>
              <use_case>Learning optimal trading policies through interaction</use_case>
            </option>
          </options>
          <selection_criteria>
            <criterion>Computational efficiency for real-time inference</criterion>
            <criterion>Interpretability for regulatory compliance</criterion>
            <criterion>Robustness to market regime changes</criterion>
            <criterion>Data requirements vs availability</criterion>
          </selection_criteria>
        </step>

        <step priority="high">
          <name>Hyperparameter optimization</name>
          <methods>
            <method>Grid search for exhaustive exploration</method>
            <method>Random search for efficiency</method>
            <method>Bayesian optimization (Optuna) for intelligent search</method>
            <method>Cross-validation for unbiased performance</method>
          </methods>
          <parameters_to_tune>
            <param>Learning rate and schedule</param>
            <param>Model complexity (depth, layers, units)</param>
            <param>Regularization (L1, L2, dropout)</param>
            <param>Batch size and sequence length</param>
            <param>Feature selection thresholds</param>
          </parameters_to_tune>
        </step>

        <step priority="critical">
          <name>Overfitting detection and prevention</name>
          <techniques>
            <technique>Monitor train vs validation metrics</technique>
            <technique>Apply early stopping with patience</technique>
            <technique>Use dropout and regularization</technique>
            <technique>Perform walk-forward validation</technique>
            <technique>Test on multiple out-of-sample periods</technique>
          </techniques>
          <red_flags>
            <flag>Training accuracy >> validation accuracy</flag>
            <flag>Perfect or near-perfect training performance</flag>
            <flag>Degrading performance on recent data</flag>
            <flag>High variance across validation folds</flag>
          </red_flags>
        </step>
      </steps>
    </phase>

    <phase name="evaluation_and_validation">
      <description>Rigorously test model performance</description>
      <steps>
        <step priority="critical">
          <name>Backtesting with realistic conditions</name>
          <requirements>
            <requirement>Include transaction costs and slippage</requirement>
            <requirement>Account for spread and market impact</requirement>
            <requirement>Simulate order execution delays</requirement>
            <requirement>Handle partial fills and rejected orders</requirement>
            <requirement>Test during different market regimes</requirement>
          </requirements>
          <metrics>
            <metric>Sharpe ratio (risk-adjusted returns)</metric>
            <metric>Maximum drawdown</metric>
            <metric>Win rate and profit factor</metric>
            <metric>Calmar ratio (return/max drawdown)</metric>
            <metric>Value at Risk (VaR) and CVaR</metric>
          </metrics>
        </step>

        <step priority="high">
          <name>Walk-forward analysis</name>
          <description>
            Simulate realistic trading by periodically retraining on expanding window
            and testing on subsequent out-of-sample period
          </description>
          <implementation>
            Use the project's walk-forward command:
            python main.py walk-forward ASSET --start-date DATE --end-date DATE --train-ratio 0.7
          </implementation>
        </step>

        <step priority="high">
          <name>Monte Carlo simulation</name>
          <description>
            Test model robustness by perturbing prices and running multiple simulations
          </description>
          <implementation>
            Use the project's monte-carlo command:
            python main.py monte-carlo ASSET --simulations 500 --noise-std 0.001
          </implementation>
          <analysis>
            Examine VaR at different confidence levels (95%, 99%)
            Check tail risk and worst-case scenarios
          </analysis>
        </step>

        <step priority="medium">
          <name>Model interpretability</name>
          <techniques>
            <technique>SHAP values for feature importance</technique>
            <technique>Attention weights for transformers</technique>
            <technique>Partial dependence plots</technique>
            <technique>Feature importance from tree models</technique>
          </techniques>
          <benefits>
            <benefit>Understand which features drive predictions</benefit>
            <benefit>Detect spurious correlations</benefit>
            <benefit>Build confidence in model behavior</benefit>
            <benefit>Meet regulatory requirements</benefit>
          </benefits>
        </step>
      </steps>
    </phase>

    <phase name="integration">
      <description>Integrate model with production trading system</description>
      <steps>
        <step priority="critical">
          <name>Decision engine integration</name>
          <locations>
            <location>finance_feedback_engine/decision_engine/engine.py</location>
            <location>finance_feedback_engine/decision_engine/ensemble_manager.py</location>
          </locations>
          <integration_points>
            <point>Add new AI provider in ensemble manager</point>
            <point>Implement query method for model inference</point>
            <point>Configure provider weights for ensemble voting</point>
            <point>Add fallback logic for model failures</point>
          </integration_points>
        </step>

        <step priority="high">
          <name>Risk management integration</name>
          <locations>
            <location>finance_feedback_engine/risk/gatekeeper.py</location>
          </locations>
          <checks>
            <check>Position sizing based on model confidence</check>
            <check>Maximum position limits</check>
            <check>Portfolio correlation constraints</check>
            <check>VaR limits enforcement</check>
            <check>Drawdown circuit breakers</check>
          </checks>
        </step>

        <step priority="high">
          <name>Monitoring and alerting</name>
          <locations>
            <location>finance_feedback_engine/monitoring/trade_monitor.py</location>
            <location>finance_feedback_engine/memory/portfolio_memory.py</location>
          </locations>
          <monitoring_targets>
            <target>Prediction accuracy vs actual outcomes</target>
            <target>Model confidence distribution</target>
            <target>Feature drift detection</target>
            <target>Inference latency</target>
            <target>Error rates and exceptions</target>
          </monitoring_targets>
        </step>
      </steps>
    </phase>

    <phase name="deployment_and_mlops">
      <description>Deploy model to production with proper MLOps practices</description>
      <steps>
        <step priority="critical">
          <name>Model versioning and registry</name>
          <actions>
            <action>Save model artifacts with version tags</action>
            <action>Track hyperparameters and training metrics</action>
            <action>Document model lineage and data dependencies</action>
            <action>Implement model loading with fallbacks</action>
          </actions>
        </step>

        <step priority="high">
          <name>A/B testing and gradual rollout</name>
          <strategy>
            <phase>Shadow mode - log predictions without trading</phase>
            <phase>Limited deployment - small position sizes</phase>
            <phase>A/B test - compare against baseline</phase>
            <phase>Full deployment - if A/B test successful</phase>
          </strategy>
        </step>

        <step priority="high">
          <name>Continuous learning pipeline</name>
          <implementation>
            <component>Data collection - store all predictions and outcomes</component>
            <component>Performance monitoring - track real-world metrics</component>
            <component>Drift detection - statistical tests for distribution changes</component>
            <component>Retraining triggers - automated or manual based on performance</component>
            <component>Model updates - deploy new versions with validation</component>
          </implementation>
          <integration>
            Leverage PortfolioMemoryEngine for storing trade outcomes
            Use TradeMetricsCollector for performance data
          </integration>
        </step>
      </steps>
    </phase>
  </main_workflow>

  <completion_criteria>
    <criterion>Model achieves target performance on holdout test set</criterion>
    <criterion>Backtest results show positive risk-adjusted returns</criterion>
    <criterion>Walk-forward validation demonstrates consistency</criterion>
    <criterion>Monte Carlo analysis shows acceptable worst-case scenarios</criterion>
    <criterion>Integration tests pass with decision engine and risk management</criterion>
    <criterion>Monitoring and alerting are configured</criterion>
    <criterion>Documentation includes model card and deployment guide</criterion>
  </completion_criteria>

  <common_pitfalls>
    <pitfall>
      <name>Data leakage</name>
      <description>Using future information in training</description>
      <prevention>Strict temporal splits, careful feature engineering</prevention>
    </pitfall>
    <pitfall>
      <name>Overfitting to historical data</name>
      <description>Model learns noise instead of signal</description>
      <prevention>Cross-validation, regularization, ensemble methods</prevention>
    </pitfall>
    <pitfall>
      <name>Ignoring transaction costs</name>
      <description>Overestimating profitability</description>
      <prevention>Include realistic costs in backtesting</prevention>
    </pitfall>
    <pitfall>
      <name>Survivorship bias</name>
      <description>Only training on assets that survived</description>
      <prevention>Include delisted assets in historical analysis</prevention>
    </pitfall>
    <pitfall>
      <name>Inadequate out-of-sample testing</name>
      <description>Optimizing on test set</description>
      <prevention>Multiple holdout periods, walk-forward validation</prevention>
    </pitfall>
  </common_pitfalls>
</ml_workflow_instructions>
