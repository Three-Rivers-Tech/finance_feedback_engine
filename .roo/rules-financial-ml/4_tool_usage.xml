<ml_tool_usage_guide>
  <overview>
    Specific guidance on using tools effectively for ML development in the
    Finance Feedback Engine project.
  </overview>

  <tool_priorities>
    <priority level="1">
      <tool>codebase_search</tool>
      <when>Always use first to find relevant ML implementations, ensemble patterns, or agent code</when>
      <why>Semantic search finds ML functionality better than keywords</why>
      <examples>
        <example>
          <query>ensemble manager multi-provider voting</query>
          <finds>EnsembleManager class with aggregation logic</finds>
        </example>
        <example>
          <query>portfolio memory learning feedback</query>
          <finds>PortfolioMemoryEngine for trade outcome tracking</finds>
        </example>
        <example>
          <query>backtesting walk-forward monte carlo</query>
          <finds>Backtesting infrastructure and validation methods</finds>
        </example>
      </examples>
    </priority>

    <priority level="2">
      <tool>read_file</tool>
      <when>After identifying files with codebase_search</when>
      <why>Get full context of ML implementations and data structures</why>
      <best_practice>Read multiple related files together (up to 5 at once)</best_practice>
    </priority>

    <priority level="3">
      <tool>execute_command</tool>
      <when>Running training scripts, backtests, or evaluations</when>
      <why>Execute long-running ML tasks</why>
      <best_practice>Use for training, validation, and performance testing</best_practice>
    </priority>
  </tool_priorities>

  <tool_specific_guidance>
    <tool name="codebase_search">
      <best_practices>
        <practice>Use domain-specific ML terminology in queries</practice>
        <practice>Search for patterns like "ensemble", "prediction", "training", "backtest"</practice>
        <practice>Include context like "decision engine" or "agent" to narrow results</practice>
      </best_practices>

      <effective_queries>
        <query_pattern type="finding_ml_models">
          <query>LSTM neural network time series prediction</query>
          <query>gradient boosting XGBoost classification</query>
          <query>reinforcement learning PPO trading agent</query>
        </query_pattern>

        <query_pattern type="finding_data_pipeline">
          <query>feature engineering technical indicators</query>
          <query>data preprocessing validation freshness</query>
          <query>multi-timeframe aggregation pulse</query>
        </query_pattern>

        <query_pattern type="finding_integration">
          <query>decision engine AI provider integration</query>
          <query>ensemble manager dynamic weights</query>
          <query>risk gatekeeper position sizing</query>
        </query_pattern>
      </effective_queries>

      <example><![CDATA[
# Finding existing ensemble implementation
<codebase_search>
<query>ensemble manager weighted voting multi-provider</query>
<path>finance_feedback_engine/decision_engine</path>
</codebase_search>

# Result guides you to ensemble_manager.py with existing patterns
      ]]></example>
    </tool>

    <tool name="read_file">
      <best_practices>
        <practice>Read related files together to understand full workflow</practice>
        <practice>Start with core modules like decision_engine and agent</practice>
        <practice>Check test files to understand expected behavior</practice>
      </best_practices>

      <ml_development_sequence>
        <step>Read decision_engine/engine.py to understand decision generation</step>
        <step>Read decision_engine/ensemble_manager.py for multi-provider logic</step>
        <step>Read agent/trading_loop_agent.py for autonomous behavior</step>
        <step>Read memory/portfolio_memory.py for learning feedback</step>
        <step>Read tests/test_ensemble*.py for testing patterns</step>
      </ml_development_sequence>

      <example><![CDATA[
# Read multiple related files for ML integration understanding
<read_file>
<args>
  <file><path>finance_feedback_engine/decision_engine/engine.py</path></file>
  <file><path>finance_feedback_engine/decision_engine/ensemble_manager.py</path></file>
  <file><path>finance_feedback_engine/memory/portfolio_memory.py</path></file>
  <file><path>tests/test_ensemble_fallback.py</path></file>
</args>
</read_file>
      ]]></example>
    </tool>

    <tool name="execute_command">
      <ml_workflows>
        <workflow name="model_training">
          <description>Train ML models using project data</description>
          <commands>
            <command purpose="train_model">
              python examples/train_ml_model.py --asset BTCUSD --start-date 2024-01-01
            </command>
            <command purpose="evaluate_model">
              python examples/evaluate_model.py --model-path models/lstm_btc.pt
            </command>
          </commands>
        </workflow>

        <workflow name="backtesting">
          <description>Validate ML models using backtesting framework</description>
          <commands>
            <command purpose="basic_backtest">
              python main.py backtest BTCUSD --start 2024-01-01 --end 2024-02-01
            </command>
            <command purpose="walk_forward">
              python main.py walk-forward BTCUSD --start-date 2024-01-01 --end-date 2024-03-01 --train-ratio 0.7
            </command>
            <command purpose="monte_carlo">
              python main.py monte-carlo BTCUSD --start-date 2024-01-01 --end-date 2024-03-01 --simulations 500
            </command>
          </commands>
        </workflow>

        <workflow name="agent_testing">
          <description>Test autonomous agent with ML models</description>
          <commands>
            <command purpose="run_agent">
              python main.py run-agent --provider ml_model --take-profit 0.05 --stop-loss 0.02
            </command>
            <command purpose="dry_run">
              python main.py run-agent --provider ml_model --trading-platform mock
            </command>
          </commands>
        </workflow>

        <workflow name="data_analysis">
          <description>Explore and analyze financial data</description>
          <commands>
            <command purpose="jupyter_notebook">
              jupyter notebook examples/ml_analysis.ipynb
            </command>
            <command purpose="feature_analysis">
              python scripts/analyze_features.py --asset BTCUSD --timeframe daily
            </command>
          </commands>
        </workflow>
      </ml_workflows>

      <best_practices>
        <practice>Set working directory appropriately using cwd parameter</practice>
        <practice>Capture output for long-running training jobs</practice>
        <practice>Use virtual environments to isolate ML dependencies</practice>
        <practice>Monitor GPU/CPU usage during training</practice>
      </best_practices>

      <example><![CDATA[
# Run model training with proper working directory
<execute_command>
<command>python examples/train_lstm_model.py --asset BTCUSD --epochs 100</command>
<cwd>/home/cmp6510/finance_feedback_engine-2.0</cwd>
</execute_command>

# Run backtesting to evaluate ML model performance
<execute_command>
<command>python main.py backtest BTCUSD --provider ml_model --start 2024-01-01</command>
</execute_command>
      ]]></example>
    </tool>

    <tool name="apply_diff">
      <ml_integration_patterns>
        <pattern name="add_ml_provider_to_ensemble">
          <description>Integrate new ML model as AI provider</description>
          <steps>
            <step>Read ensemble_manager.py to understand structure</step>
            <step>Apply diff to add provider initialization</step>
            <step>Apply diff to add provider to query methods</step>
            <step>Update configuration to include provider weights</step>
          </steps>
        </pattern>

        <pattern name="add_feature_engineering">
          <description>Add new technical indicators</description>
          <files_to_modify>
            <file>finance_feedback_engine/data_providers/timeframe_aggregator.py</file>
            <file>finance_feedback_engine/data_providers/unified_data_provider.py</file>
          </files_to_modify>
        </pattern>

        <pattern name="enhance_risk_management">
          <description>Add ML-based risk scoring</description>
          <files_to_modify>
            <file>finance_feedback_engine/risk/gatekeeper.py</file>
          </files_to_modify>
        </pattern>
      </ml_integration_patterns>
    </tool>

    <tool name="write_to_file">
      <note>
        In financial-ml mode, you can create new Python files for ML models, training scripts,
        and examples. Use write_to_file for complete new implementations.
      </note>

      <common_use_cases>
        <use_case>Creating new ML model architectures</use_case>
        <use_case>Writing training scripts</use_case>
        <use_case>Creating data preprocessing utilities</use_case>
        <use_case>Building model evaluation notebooks</use_case>
        <use_case>Implementing custom loss functions</use_case>
      </common_use_cases>

      <file_organization>
        <location>finance_feedback_engine/decision_engine/ - AI provider implementations</location>
        <location>finance_feedback_engine/models/ - Model architectures (create if needed)</location>
        <location>examples/ - Training and evaluation scripts</location>
        <location>tests/ - Model and integration tests</location>
      </file_organization>
    </tool>

    <tool name="ask_followup_question">
      <when_to_use>
        <scenario>Unclear which ML architecture to use (LSTM vs Transformer vs GBM)</scenario>
        <scenario>Need hyperparameter guidance from user</scenario>
        <scenario>Uncertain about training data time range</scenario>
        <scenario>Need to confirm deployment strategy (shadow mode vs production)</scenario>
      </when_to_use>

      <ml_specific_questions>
        <question type="model_selection">
          <text>Which ML architecture would you like to use for this task?</text>
          <suggestions>
            <suggest>LSTM neural network for time series prediction</suggest>
            <suggest>XGBoost/LightGBM for feature-based classification</suggest>
            <suggest>Ensemble combining multiple model types</suggest>
            <suggest>Reinforcement learning agent (PPO/A2C)</suggest>
          </suggestions>
        </question>

        <question type="training_data">
          <text>What time range should we use for training the model?</text>
          <suggestions>
            <suggest>Last 6 months (sufficient for most patterns)</suggest>
            <suggest>Last 1 year (captures seasonal patterns)</suggest>
            <suggest>Last 2 years (includes multiple market cycles)</suggest>
            <suggest>Custom range (specify dates)</suggest>
          </suggestions>
        </question>

        <question type="deployment">
          <text>How should we deploy the trained model?</text>
          <suggestions>
            <suggest>Shadow mode first (log predictions without trading)</suggest>
            <suggest>Small position sizes with mock platform testing</suggest>
            <suggest>Add to ensemble with low initial weight (10%)</suggest>
            <suggest>Replace existing provider after validation</suggest>
          </suggestions>
        </question>
      </ml_specific_questions>

      <example><![CDATA[
<ask_followup_question>
<question>What type of ML model would work best for this price prediction task?</question>
<follow_up>
<suggest>LSTM neural network for capturing temporal dependencies in price movements</suggest>
<suggest>XGBoost gradient boosting for fast training on engineered features</suggest>
<suggest>Stacked ensemble combining LSTM, XGBoost, and Random Forest</suggest>
<suggest>Reinforcement learning agent that learns optimal trading policy</suggest>
</follow_up>
</ask_followup_question>
      ]]></example>
    </tool>
  </tool_specific_guidance>

  <testing_workflow>
    <step number="1">
      <action>Unit test model components</action>
      <command>pytest tests/test_ml_model.py -v</command>
    </step>

    <step number="2">
      <action>Integration test with decision engine</action>
      <command>pytest tests/test_decision_engine_ml.py -v</command>
    </step>

    <step number="3">
      <action>Backtest on historical data</action>
      <command>python main.py backtest BTCUSD --provider ml_model</command>
    </step>

    <step number="4">
      <action>Walk-forward validation</action>
      <command>python main.py walk-forward BTCUSD --train-ratio 0.7</command>
    </step>

    <step number="5">
      <action>Monte Carlo robustness test</action>
      <command>python main.py monte-carlo BTCUSD --simulations 500</command>
    </step>
  </testing_workflow>

  <common_workflows>
    <workflow name="develop_new_ml_model">
      <step>Use codebase_search to find similar implementations</step>
      <step>Read relevant files (ensemble_manager, existing models)</step>
      <step>Create new model file with write_to_file</step>
      <step>Write unit tests for model</step>
      <step>Execute training command</step>
      <step>Run backtests to validate</step>
      <step>Integrate with ensemble using apply_diff</step>
      <step>Deploy in shadow mode first</step>
    </workflow>

    <workflow name="optimize_existing_model">
      <step>Use codebase_search to locate model</step>
      <step>Read model implementation and training code</step>
      <step>Execute hyperparameter tuning</step>
      <step>Compare results with baseline</step>
      <step>Update model using apply_diff if improvements found</step>
      <step>Re-run validation suite</step>
    </workflow>

    <workflow name="debug_model_performance">
      <step>Check training/validation metrics</step>
      <step>Analyze feature importance</step>
      <step>Review backtest results for issues</step>
      <step>Test on different market regimes</step>
      <step>Check for data leakage or overfitting</step>
      <step>Implement fixes and re-validate</step>
    </workflow>
  </common_workflows>
</ml_tool_usage_guide>
