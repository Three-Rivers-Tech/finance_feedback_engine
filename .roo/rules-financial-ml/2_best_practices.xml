<ml_best_practices>
  <general_principles>
    <principle priority="critical">
      <name>Temporal Integrity</name>
      <description>
        Always respect the temporal order of financial data. Never use future information
        to predict past events (look-ahead bias).
      </description>
      <rationale>
        Financial markets are non-stationary and time-dependent. Using future data
        creates unrealistic backtests and will fail in production.
      </rationale>
      <implementation>
        <rule>Use only lagged features (t-1, t-2, etc.) to predict time t</rule>
        <rule>Split data chronologically, never randomly shuffle</rule>
        <rule>Validate that all feature calculations use only past data</rule>
        <rule>Be careful with pandas operations that might peek forward</rule>
      </implementation>
      <example>
        <scenario>Calculating rolling statistics</scenario>
        <good><![CDATA[
# Correct: Uses only past data
df['sma_20'] = df['close'].rolling(window=20, min_periods=20).mean()
df['returns'] = df['close'].pct_change()
df['volatility'] = df['returns'].rolling(window=20).std()
        ]]></good>
        <bad><![CDATA[
# Wrong: Forward-looking calculation
df['future_return'] = df['close'].shift(-1) / df['close'] - 1  # LOOK-AHEAD BIAS!
df['normalized'] = (df['close'] - df['close'].mean()) / df['close'].std()  # Uses future data!
        ]]></bad>
      </example>
    </principle>

    <principle priority="critical">
      <name>Realistic Backtesting</name>
      <description>
        Include all real-world constraints: transaction costs, slippage, spreads,
        order execution delays, and market impact.
      </description>
      <rationale>
        Paper profits without costs can be completely eliminated by realistic trading expenses.
        A strategy profitable in simulation may lose money in production.
      </rationale>
      <implementation>
        <rule>Add commission fees (e.g., 0.1% per trade)</rule>
        <rule>Model slippage based on order size and volatility</rule>
        <rule>Include bid-ask spreads</rule>
        <rule>Simulate execution delays (1-5 seconds)</rule>
        <rule>Account for market impact on large orders</rule>
      </implementation>
      <example>
        <scenario>Calculating trade P&L</scenario>
        <good><![CDATA[
# Realistic P&L calculation
entry_price = price * (1 + spread/2)  # Buy at ask
exit_price = price * (1 - spread/2)   # Sell at bid
commission = position_size * commission_rate
slippage = position_size * slippage_rate
net_pnl = (exit_price - entry_price) * shares - commission - slippage
        ]]></good>
        <bad><![CDATA[
# Unrealistic P&L - no costs
net_pnl = (exit_price - entry_price) * shares  # Missing costs!
        ]]></bad>
      </example>
    </principle>

    <principle priority="high">
      <name>Robust Validation</name>
      <description>
        Use multiple validation techniques to ensure model generalization:
        walk-forward analysis, out-of-sample testing, Monte Carlo simulation.
      </description>
      <rationale>
        Single train/test splits can give misleading results. Models must work
        across different market regimes and time periods.
      </rationale>
      <implementation>
        <rule>Perform walk-forward validation with multiple windows</rule>
        <rule>Test on completely unseen recent data</rule>
        <rule>Run Monte Carlo simulations with price perturbations</rule>
        <rule>Validate across different market regimes</rule>
        <rule>Use expanding window cross-validation for time series</rule>
      </implementation>
    </principle>

    <principle priority="high">
      <name>Feature Engineering Quality</name>
      <description>
        Create meaningful features that capture market dynamics while avoiding
        overfitting and maintaining computational efficiency.
      </description>
      <rationale>
        Quality features are the foundation of successful ML models. Poor features
        lead to overfitting and unreliable predictions.
      </rationale>
      <implementation>
        <rule>Use domain knowledge to guide feature creation</rule>
        <rule>Test features individually before combining</rule>
        <rule>Remove highly correlated features (correlation > 0.95)</rule>
        <rule>Standardize or normalize features for consistency</rule>
        <rule>Handle missing values explicitly (don't just forward-fill)</rule>
      </implementation>
    </principle>
  </general_principles>

  <code_conventions>
    <convention category="data_handling">
      <rule>Always use pandas with timezone-aware datetimes</rule>
      <rule>Store prices as float64, volumes as int64</rule>
      <rule>Use ISO 8601 format for timestamps</rule>
      <rule>Validate data quality before model training</rule>
      <examples>
        <good><![CDATA[
# Proper data loading with validation
import pandas as pd
from finance_feedback_engine.utils.validation import validate_data_freshness

df = pd.read_csv('data.csv', parse_dates=['timestamp'])
df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)
df = df.set_index('timestamp').sort_index()

# Validate freshness
is_fresh, age, msg = validate_data_freshness(
    df.index[-1].isoformat(),
    asset_type="crypto"
)
if not is_fresh:
    raise ValueError(f"Stale data: {msg}")
        ]]></good>
      </examples>
    </convention>

    <convention category="model_training">
      <rule>Set random seeds for reproducibility</rule>
      <rule>Use early stopping to prevent overfitting</rule>
      <rule>Log all hyperparameters and metrics</rule>
      <rule>Save model checkpoints during training</rule>
      <examples>
        <good><![CDATA[
import numpy as np
import torch
import random

# Set seeds for reproducibility
def set_seeds(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seeds(42)

# Training with early stopping
from torch.optim import Adam
from torch.nn import MSELoss

optimizer = Adam(model.parameters(), lr=0.001)
criterion = MSELoss()
early_stop_patience = 10
best_val_loss = float('inf')
patience_counter = 0

for epoch in range(num_epochs):
    train_loss = train_epoch(model, train_loader, criterion, optimizer)
    val_loss = validate(model, val_loader, criterion)

    # Early stopping logic
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        torch.save(model.state_dict(), 'best_model.pt')
    else:
        patience_counter += 1
        if patience_counter >= early_stop_patience:
            print(f"Early stopping at epoch {epoch}")
            break
        ]]></good>
      </examples>
    </convention>

    <convention category="ensemble_integration">
      <rule>Follow existing ensemble manager patterns</rule>
      <rule>Implement proper fallback logic</rule>
      <rule>Return structured decision objects</rule>
      <rule>Handle provider failures gracefully</rule>
      <examples>
        <good><![CDATA[
# Proper ensemble provider implementation
class MLModelProvider:
    def __init__(self, model_path, config):
        self.model = self.load_model(model_path)
        self.config = config

    def query(self, prompt_data):
        """Query ML model for trading decision."""
        try:
            features = self.extract_features(prompt_data)
            prediction = self.model.predict(features)
            confidence = self.calculate_confidence(prediction)

            return {
                "action": self.map_to_action(prediction),
                "confidence": confidence,
                "reasoning": self.generate_reasoning(prediction, features),
                "metadata": {
                    "model_version": self.config['version'],
                    "features_used": list(features.keys())
                }
            }
        except Exception as e:
            # Log error and return failure indicator
            logger.error(f"ML model query failed: {e}")
            return None  # Ensemble manager will handle fallback

    def calculate_confidence(self, prediction):
        """Convert model output to confidence score [0, 1]."""
        # Implement based on model type
        if hasattr(prediction, 'probability'):
            return float(prediction.probability.max())
        else:
            # For regression, use prediction stability
            return min(1.0, abs(prediction) / self.config['max_expected_value'])
        ]]></good>
      </examples>
    </convention>

    <convention category="testing">
      <rule>Write tests for data preprocessing pipelines</rule>
      <rule>Test models on synthetic data first</rule>
      <rule>Mock external dependencies in unit tests</rule>
      <rule>Include integration tests with decision engine</rule>
      <examples>
        <good><![CDATA[
import pytest
from finance_feedback_engine.decision_engine.ml_provider import MLModelProvider

def test_ml_provider_basic_prediction():
    """Test basic ML provider functionality."""
    provider = MLModelProvider(
        model_path="tests/fixtures/test_model.pkl",
        config={"version": "1.0", "max_expected_value": 0.1}
    )

    prompt_data = {
        "asset_pair": "BTCUSD",
        "current_price": 50000,
        "features": {
            "rsi_14": 65.2,
            "macd": 0.002,
            "volatility": 0.03
        }
    }

    result = provider.query(prompt_data)

    assert result is not None
    assert "action" in result
    assert result["action"] in ["BUY", "SELL", "HOLD"]
    assert 0 <= result["confidence"] <= 1
    assert "reasoning" in result

def test_ml_provider_handles_missing_features():
    """Test graceful handling of missing features."""
    provider = MLModelProvider(...)

    # Missing critical features
    prompt_data = {"asset_pair": "BTCUSD"}

    result = provider.query(prompt_data)

    # Should return None or default, not crash
    assert result is None or result["action"] == "HOLD"
        ]]></good>
      </examples>
    </convention>
  </code_conventions>

  <quality_checklist>
    <category name="before_training">
      <item>Data loaded correctly with proper timestamps</item>
      <item>No missing values or addressed explicitly</item>
      <item>Features calculated without look-ahead bias</item>
      <item>Train/validation/test splits are chronological</item>
      <item>Data normalized/standardized consistently</item>
      <item>No data leakage between splits</item>
    </category>

    <category name="during_training">
      <item>Random seeds set for reproducibility</item>
      <item>Hyperparameters logged</item>
      <item>Training and validation metrics tracked</item>
      <item>Early stopping configured</item>
      <item>Model checkpoints saved</item>
      <item>Overfitting monitored (train vs val gap)</item>
    </category>

    <category name="after_training">
      <item>Model evaluated on holdout test set</item>
      <item>Backtest includes transaction costs</item>
      <item>Walk-forward validation performed</item>
      <item>Monte Carlo simulation run</item>
      <item>Feature importance analyzed</item>
      <item>Model performance documented</item>
    </category>

    <category name="before_deployment">
      <item>Integration tests with decision engine pass</item>
      <item>Risk management validation complete</item>
      <item>Error handling tested</item>
      <item>Monitoring and alerting configured</item>
      <item>Model card documented</item>
      <item>Rollback plan prepared</item>
    </category>
  </quality_checklist>

  <common_antipatterns>
    <antipattern>
      <name>Training on entire dataset</name>
      <description>Using all available data for training without holdout</description>
      <why_bad>Cannot assess true generalization performance</why_bad>
      <correct_approach>Reserve 15-20% of recent data for final testing</correct_approach>
    </antipattern>

    <antipattern>
      <name>Optimizing hyperparameters on test set</name>
      <description>Tuning based on test performance</description>
      <why_bad>Test set becomes part of training, overfitting guaranteed</why_bad>
      <correct_approach>Use separate validation set for hyperparameter tuning</correct_approach>
    </antipattern>

    <antipattern>
      <name>Ignoring market regimes</name>
      <description>Training single model for all market conditions</description>
      <why_bad>Models optimized for trending markets fail in ranging markets</why_bad>
      <correct_approach>Train separate models for different regimes or use regime as feature</correct_approach>
    </antipattern>

    <antipattern>
      <name>Using default hyperparameters</name>
      <description>Not tuning model for specific task</description>
      <why_bad>Default settings rarely optimal for financial data</why_bad>
      <correct_approach>Perform systematic hyperparameter search</correct_approach>
    </antipattern>

    <antipattern>
      <name>Ignoring class imbalance</name>
      <description>Training on unbalanced buy/sell/hold signals</description>
      <why_bad>Model biased toward majority class</why_bad>
      <correct_approach>Use weighted loss, SMOTE, or adjust decision threshold</correct_approach>
    </antipattern>
  </common_antipatterns>

  <performance_considerations>
    <consideration category="inference_speed">
      <requirement>Real-time trading requires sub-second predictions</requirement>
      <guidelines>
        <guideline>Optimize model architecture for inference</guideline>
        <guideline>Use model quantization if needed</guideline>
        <guideline>Cache repeated calculations</guideline>
        <guideline>Consider ensemble size vs latency tradeoff</guideline>
      </guidelines>
    </consideration>

    <consideration category="memory_usage">
      <requirement>Models must fit in available RAM</requirement>
      <guidelines>
        <guideline>Use appropriate data types (float32 vs float64)</guideline>
        <guideline>Implement feature selection to reduce dimensions</guideline>
        <guideline>Stream data for large datasets</guideline>
        <guideline>Clear cached data after training</guideline>
      </guidelines>
    </consideration>

    <consideration category="retraining_frequency">
      <requirement>Balance freshness vs computational cost</requirement>
      <guidelines>
        <guideline>Monitor model drift to trigger retraining</guideline>
        <guideline>Implement incremental learning if possible</guideline>
        <guideline>Schedule retraining during off-peak hours</guideline>
        <guideline>Validate new model before deployment</guideline>
      </guidelines>
    </consideration>
  </performance_considerations>
</ml_best_practices>
